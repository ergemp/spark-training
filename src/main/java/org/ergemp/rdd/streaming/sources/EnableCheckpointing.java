package org.ergemp.rdd.streaming.sources;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function0;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import java.util.Arrays;

public class EnableCheckpointing {

    private static  String checkpointDirectory = "spark/checkpoint/EnableCheckpointingExample";

    private static JavaStreamingContext createContext(String ip,
                                                      int port,
                                                      String checkpointDirectory) {

        // If you do not see this printed, that means the StreamingContext has been loaded
        // from the new checkpoint
        System.out.println("Creating new context");

        SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("EnableCheckpointingExample");

        // Create the context with 10 seconds of batch size
        JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(10));
        ssc.checkpoint(checkpointDirectory);

        // Create a socket stream on target ip:port and count the
        // words in input stream of \n delimited text (e.g. generated by 'nc')
        JavaReceiverInputDStream<String> lines = ssc.socketTextStream(ip, port);

        JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());

        words.foreachRDD((rdd, time) -> {
            rdd.foreach(each -> System.out.println(each));
        });

        return ssc;
    }


    public static void main(String[] args) throws InterruptedException {

        /*
        *
        * Checkpointing must be enabled for applications with any of the following requirements:
        *
        * Usage of stateful transformations - If either updateStateByKey or reduceByKeyAndWindow
        *       (with inverse function) is used in the application, then the checkpoint directory
        *       must be provided to allow for periodic RDD checkpointing.
        *
        * Recovering from failures of the driver running the application - Metadata checkpoints are used
        *       to recover with progress information.
        *
        *
        * Additionally, if you want to make the application recover from driver failures, you should rewrite your streaming application to have the following behavior.
        *
        * When the program is being started for the first time, it will create a new StreamingContext,
        *   set up all the streams and then call start().
        *
        * When the program is being restarted after failure, it will re-create a StreamingContext
        *   from the checkpoint data in the checkpoint directory.
        *
        *
        * */


        // Get JavaStreamingContext from checkpoint data or create a new one
        // Function to create JavaStreamingContext without any output operations
        // (used to detect the new context)
        Function0<JavaStreamingContext> createContextFunc =
                () -> createContext("localhost", 19999, checkpointDirectory);

        JavaStreamingContext ssc = JavaStreamingContext.getOrCreate(checkpointDirectory, createContextFunc);

        ssc.start();
        ssc.awaitTermination();

        /* If the checkpointDirectory exists,
         * then the context will be recreated from the checkpoint data.
         *
         * If the directory does not exist (i.e., running for the first time),
         * then the function contextFactory will be called to create a new context and set up the DStreams.
         * */

    }
}
